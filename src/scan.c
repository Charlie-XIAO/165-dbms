/**
 * @file scan.c
 * @implements scan.h
 */

#include <assert.h>
#include <limits.h>
#include <stdlib.h>
#include <string.h>

#include "consts.h"
#include "logging.h"
#include "scan.h"
#include "sysinfo.h"
#include "thread_pool.h"

/**
 * Helper macro to define a single SELECT iteration.
 */
#define _SHARED_SCAN_SELECT_ITER(VALUE, POSITION, CTX, FLAGS)                  \
  do {                                                                         \
    if (FLAGS & SCAN_CALLBACK_SELECT_FLAG) {                                   \
      for (size_t _c = 0; _c < CTX->n_select_queries; _c++) {                  \
        if (VALUE >= CTX->lower_bound_arr[_c] &&                               \
            VALUE < CTX->upper_bound_arr[_c]) {                                \
          CTX->selected_indices_arr_flattened                                  \
              [ctx->n_selected_indices_arr[_c]++ * ctx->n_select_queries +     \
               _c] = POSITION;                                                 \
        }                                                                      \
      }                                                                        \
    }                                                                          \
  } while (0)

/**
 * Helper macro to define a single MIN iteration.
 */
#define _SHARED_SCAN_MIN_ITER(VALUE, CTX, FLAGS)                               \
  do {                                                                         \
    if (FLAGS & SCAN_CALLBACK_MIN_FLAG) {                                      \
      if (VALUE < CTX->min_result) {                                           \
        CTX->min_result = VALUE;                                               \
      }                                                                        \
    }                                                                          \
  } while (0)

/**
 * Helper macro to define a single MAX iteration.
 */
#define _SHARED_SCAN_MAX_ITER(VALUE, CTX, FLAGS)                               \
  do {                                                                         \
    if (FLAGS & SCAN_CALLBACK_MAX_FLAG) {                                      \
      if (VALUE > CTX->max_result) {                                           \
        CTX->max_result = VALUE;                                               \
      }                                                                        \
    }                                                                          \
  } while (0)

/**
 * Helper macro to define a single SUM iteration.
 */
#define _SHARED_SCAN_SUM_ITER(VALUE, CTX, FLAGS)                               \
  do {                                                                         \
    if (FLAGS & SCAN_CALLBACK_SUM_FLAG) {                                      \
      CTX->sum_result += VALUE;                                                \
    }                                                                          \
  } while (0)

/**
 * Shared scan function for a specific combination of scan operations.
 *
 * This function is generated by a macro for a specific combination of scan
 * operations. Consider a single function that handles all possible combinations
 * of scan operations; it needs to dynamically determine the operations to
 * perform based on the input flags, which incurs a runtime overhead because of
 * the if statements in each iteration. On the other hand, this generated
 * function is dedicated to a specific combination of scan operations, causing
 * the if statements to be resolved at compile time, thus mitigating the runtime
 * overhead.
 */
#define _SHARED_SCAN(FLAGS)                                                    \
  void shared_scan_##FLAGS(GeneralizedValvec *valvec,                          \
                           GeneralizedPosvec *posvec, ScanContext *ctx,        \
                           size_t start, size_t end) {                         \
    int *data = valvec->valvec_type == GENERALIZED_VALVEC_TYPE_COLUMN          \
                    ? valvec->valvec_pointer.column->data                      \
                    : valvec->valvec_pointer.partial_column->values;           \
                                                                               \
    if (posvec == NULL) {                                                      \
      for (size_t i = start; i < end; i++) {                                   \
        _SHARED_SCAN_SELECT_ITER(data[i], i, ctx, FLAGS);                      \
        _SHARED_SCAN_MIN_ITER(data[i], ctx, FLAGS);                            \
        _SHARED_SCAN_MAX_ITER(data[i], ctx, FLAGS);                            \
        _SHARED_SCAN_SUM_ITER(data[i], ctx, FLAGS);                            \
      }                                                                        \
      return;                                                                  \
    }                                                                          \
                                                                               \
    for (size_t i = start; i < end; i++) {                                     \
      size_t index = posvec->posvec_pointer.index_array->indices[i];           \
      _SHARED_SCAN_SELECT_ITER(data[i], index, ctx, FLAGS);                    \
    }                                                                          \
  }

_SHARED_SCAN(0x01)
_SHARED_SCAN(0x02)
_SHARED_SCAN(0x03)
_SHARED_SCAN(0x04)
_SHARED_SCAN(0x05)
_SHARED_SCAN(0x06)
_SHARED_SCAN(0x07)
_SHARED_SCAN(0x08)
_SHARED_SCAN(0x09)
_SHARED_SCAN(0x0a)
_SHARED_SCAN(0x0b)
_SHARED_SCAN(0x0c)
_SHARED_SCAN(0x0d)
_SHARED_SCAN(0x0e)
_SHARED_SCAN(0x0f)

/**
 * @implements init_empty_scan_context
 */
ScanContext init_empty_scan_context() {
  return (ScanContext){
      .lower_bound_arr = NULL,
      .upper_bound_arr = NULL,
      .selected_indices_arr = NULL,
      .n_selected_indices_arr = NULL,
      .n_select_queries = 0,
      .min_result = INT_MAX,
      .max_result = INT_MIN,
      .sum_result = 0,
  };
}

/**
 * @implements shared_scan_worker
 */
void shared_scan_subroutine(SharedScanTaskData *task_data) {
  task_data->shared_scan_func(task_data->valvec, task_data->posvec,
                              task_data->ctx, task_data->start, task_data->end);
}

/**
 * Helper function to perform shared scan sequentially.
 *
 * This function will return the status code of the operation.
 */
DbSchemaStatus _shared_scan_sequential(SharedScanFunc shared_scan_func,
                                       GeneralizedValvec *valvec,
                                       GeneralizedPosvec *posvec,
                                       ScanContext *ctx, int flags) {
  // Pre-processing for SELECT
  // 1. Allocate memory for `n_selected_indices_arr`
  // 2. Allocate memory for `selected_indices_arr_flattened`; this is going to
  //    be used internally and in post-processing it will be converted to the
  //    final `selected_indices_arr` and freed
  if (flags & SCAN_CALLBACK_SELECT_FLAG) {
    ctx->n_selected_indices_arr = calloc(ctx->n_select_queries, sizeof(size_t));
    if (ctx->n_selected_indices_arr == NULL) {
      return DB_SCHEMA_STATUS_ALLOC_FAILED;
    }
    ctx->selected_indices_arr_flattened =
        malloc(valvec->valvec_length * ctx->n_select_queries * sizeof(size_t));
    if (ctx->selected_indices_arr_flattened == NULL) {
      free(ctx->n_selected_indices_arr);
      return DB_SCHEMA_STATUS_ALLOC_FAILED;
    }
  }

  // Dispatch the shared scan function directly in the main thread, for the
  // entire value vector
  shared_scan_func(valvec, posvec, ctx, 0, valvec->valvec_length);

  // Post-processing for SELECT
  // 1. Allocate memory for `selected_indices_arr`, and for each subarray with
  //    just enough capacity (obtained from the selected indices count)
  // 2. Copy the selected indices from the flattened array to the each subarray
  if (flags & SCAN_CALLBACK_SELECT_FLAG) {
    ctx->selected_indices_arr =
        malloc(ctx->n_select_queries * sizeof(size_t *));
    if (ctx->selected_indices_arr == NULL) {
      free(ctx->n_selected_indices_arr);
      free(ctx->selected_indices_arr_flattened);
      return DB_SCHEMA_STATUS_ALLOC_FAILED;
    }

    for (size_t i = 0; i < ctx->n_select_queries; i++) {
      // Allocate just enough capacity for each selected indices array
      ctx->selected_indices_arr[i] =
          malloc(ctx->n_selected_indices_arr[i] * sizeof(size_t));
      if (ctx->selected_indices_arr[i] == NULL) {
        for (size_t j = 0; j < i; j++) {
          free(ctx->selected_indices_arr[j]);
        }
        free(ctx->n_selected_indices_arr);
        free(ctx->selected_indices_arr);
        free(ctx->selected_indices_arr_flattened);
        return DB_SCHEMA_STATUS_ALLOC_FAILED;
      }
      // Copy the selected indices from the flattened array to the new array
      for (size_t j = 0; j < ctx->n_selected_indices_arr[i]; j++) {
        ctx->selected_indices_arr[i][j] =
            ctx->selected_indices_arr_flattened[j * ctx->n_select_queries + i];
      }
    }

    free(ctx->selected_indices_arr_flattened);
  }

  return DB_SCHEMA_STATUS_OK;
}

/**
 * Helper function to perform shared scan in parallel.
 *
 * This function essentially splits and dispatches jobs to the thread pool,
 * while the actual processing is handled by the shared scan worker function.
 * This function will return the status code of the operation.
 */
DbSchemaStatus _shared_scan_parallel(SharedScanFunc shared_scan_func,
                                     GeneralizedValvec *valvec,
                                     GeneralizedPosvec *posvec,
                                     ScanContext *ctx, int flags) {
  size_t total_length = valvec->valvec_length;

  // Break the shared scan into parts, each of page size
  int offset = __page_size__ * NUM_PAGES_PER_SCAN_TASK / sizeof(int);
  int n_tasks = total_length / offset + (total_length % offset != 0);
  ScanContext subctxs[n_tasks];

  // Pre-processing for SELECT (overall)
  // 1. Allocate memory for `n_selected_indices_arr`; however, we do not just
  //    allocate `n_select_queries` elements because each task should increment
  //    its own count to avoid interference; hence we allocate `n_tasks` many
  //    times the size of `n_select_queries`
  // 2. Allocate memory for `selected_indices_arr_flattened`; this is going to
  //    be used internally and in post-processing it will be converted to the
  //    final `selected_indices_arr` and freed; different tasks would be writing
  //    to different (contiguous) parts of this array
  if (flags & SCAN_CALLBACK_SELECT_FLAG) {
    ctx->n_selected_indices_arr =
        calloc(ctx->n_select_queries * n_tasks, sizeof(size_t));
    if (ctx->n_selected_indices_arr == NULL) {
      return DB_SCHEMA_STATUS_ALLOC_FAILED;
    }
    ctx->selected_indices_arr_flattened =
        malloc(valvec->valvec_length * ctx->n_select_queries * sizeof(size_t));
    if (ctx->selected_indices_arr_flattened == NULL) {
      free(ctx->n_selected_indices_arr);
      return DB_SCHEMA_STATUS_ALLOC_FAILED;
    }
  }

  int i = 0;
  thread_pool_reset_queue_completion(__thread_pool__);
  for (size_t start = 0; start < total_length; start += offset) {
    size_t end = start + offset < total_length ? start + offset : total_length;

    // Initialize subcontext for the part; we first shallow copy the original
    // context and adjust on top of it
    subctxs[i] = *ctx;

    // Pre-processing for SELECT (per task)
    // 1. Add offset to `n_selected_indices_arr`
    // 2. Add offset to `selected_indices_arr_flattened`
    if (flags & SCAN_CALLBACK_SELECT_FLAG) {
      subctxs[i].n_selected_indices_arr =
          ctx->n_selected_indices_arr + i * ctx->n_select_queries;
      subctxs[i].selected_indices_arr_flattened =
          ctx->selected_indices_arr_flattened + start * ctx->n_select_queries;
    }

    // Wrap into a thread task and enqueue
    SharedScanTaskData *task_data = malloc(sizeof(SharedScanTaskData));
    if (task_data == NULL) {
      return DB_SCHEMA_STATUS_ALLOC_FAILED;
    }
    task_data->shared_scan_func = shared_scan_func;
    task_data->valvec = valvec;
    task_data->posvec = posvec;
    task_data->start = start;
    task_data->end = end;
    task_data->ctx = &subctxs[i];
    ThreadTask task = {.id = next_task_id(),
                       .type = THREAD_TASK_TYPE_SHARED_SCAN,
                       .data = task_data};
    thread_pool_enqueue_task(__thread_pool__, &task);
    i++;

    log_file(stdout, "  [LOG] Enqueued shared scan task %d\n", task.id);
  }

  // Wait until the task queue is emptied; since this function is the only
  // producer to the shared scan task queue, waiting until the task queue is
  // emptied is equivalent to waiting until our enqueued tasks are completed;
  // XXX: perhaps post-processing per task completion (in order) would be more
  // efficient than waiting for all tasks to complete in the first place
  thread_pool_wait_queue_completion(__thread_pool__, n_tasks);
  log_file(stdout, "  [LOG] Shared scans completed\n");

  // Post-processing for SELECT
  // 1. Sum aggregate `n_selected_indices_arr` from the subcontexts into a
  //    single array containing the total counts per select query
  // 2. Allocate memory for `selected_indices_arr`, and for each subarray with
  //    just enough capacity (obtained from the selected aggregated counts)
  // 3. Copy the selected indices from the flattened array to the each subarray;
  if (flags & SCAN_CALLBACK_SELECT_FLAG) {
    size_t *merged_n_selected_indices_arr =
        calloc(ctx->n_select_queries, sizeof(size_t));
    if (merged_n_selected_indices_arr == NULL) {
      free(ctx->n_selected_indices_arr);
      free(ctx->selected_indices_arr_flattened);
      return DB_SCHEMA_STATUS_ALLOC_FAILED;
    }
    ctx->selected_indices_arr =
        malloc(ctx->n_select_queries * sizeof(size_t *));
    if (ctx->selected_indices_arr == NULL) {
      free(merged_n_selected_indices_arr);
      free(ctx->n_selected_indices_arr);
      free(ctx->selected_indices_arr_flattened);
      return DB_SCHEMA_STATUS_ALLOC_FAILED;
    }

    // Merge the counts from each subcontext
    for (int i = 0; i < n_tasks; i++) {
      for (size_t j = 0; j < ctx->n_select_queries; j++) {
        merged_n_selected_indices_arr[j] +=
            subctxs[i].n_selected_indices_arr[j];
      }
    }

    // Allocate just enough capacity for each selected indices array
    for (size_t j = 0; j < ctx->n_select_queries; j++) {
      ctx->selected_indices_arr[j] =
          malloc(merged_n_selected_indices_arr[j] * sizeof(size_t));
      if (ctx->selected_indices_arr[j] == NULL) {
        for (size_t k = 0; k < j; k++) {
          free(ctx->selected_indices_arr[k]);
        }
        free(merged_n_selected_indices_arr);
        free(ctx->n_selected_indices_arr);
        free(ctx->selected_indices_arr);
        free(ctx->selected_indices_arr_flattened);
        return DB_SCHEMA_STATUS_ALLOC_FAILED;
      }
    }

    // Copy the selected indices from the flattened array to the new array; note
    // that even though making `j` the outermost loop would mitigate the need of
    // the `cp_offsets` array, it would be less cache-friendly
    size_t cp_offsets[ctx->n_select_queries];
    memset(cp_offsets, 0, sizeof(cp_offsets));
    for (int i = 0; i < n_tasks; i++) {
      for (size_t j = 0; j < ctx->n_select_queries; j++) {
        for (size_t k = 0; k < subctxs[i].n_selected_indices_arr[j]; k++) {
          ctx->selected_indices_arr[j][cp_offsets[j] + k] =
              subctxs[i]
                  .selected_indices_arr_flattened[k * ctx->n_select_queries +
                                                  j];
        }
        cp_offsets[j] += subctxs[i].n_selected_indices_arr[j];
      }
    }

    // Free intermediate memory and update the context
    free(ctx->selected_indices_arr_flattened);
    free(ctx->n_selected_indices_arr);
    ctx->selected_indices_arr_flattened = NULL;
    ctx->n_selected_indices_arr = merged_n_selected_indices_arr;
  }

  // Post-processing for MIN (min reduction)
  if (flags & SCAN_CALLBACK_MIN_FLAG) {
    for (int i = 0; i < n_tasks; i++) {
      if (subctxs[i].min_result < ctx->min_result) {
        ctx->min_result = subctxs[i].min_result;
      }
    }
  }

  // Post-processing for MAX (max reduction)
  if (flags & SCAN_CALLBACK_MAX_FLAG) {
    for (int i = 0; i < n_tasks; i++) {
      if (subctxs[i].max_result > ctx->max_result) {
        ctx->max_result = subctxs[i].max_result;
      }
    }
  }

  // Post-processing for SUM (sum reduction)
  if (flags & SCAN_CALLBACK_SUM_FLAG) {
    for (int i = 0; i < n_tasks; i++) {
      ctx->sum_result += subctxs[i].sum_result;
    }
  }

  return DB_SCHEMA_STATUS_OK;
}

/**
 * @implements shared_scan
 */
DbSchemaStatus shared_scan(GeneralizedValvec *valvec, GeneralizedPosvec *posvec,
                           ScanContext *ctx, int flags) {
  if (__multi_threaded__ && __thread_pool__ == NULL) {
    // If multi-threading is enabled but the thread pool is initialized (because
    // we specified zero workers), this would be an error; in all other cases
    // whether to do parallelized shared scan depends on __multi_threaded__
    return DB_SCHEMA_STATUS_PARALLEL_NOT_INITIALIZED;
  }

  // Determine the shared scan function and meanwhile validate the flags
  SharedScanFunc shared_scan_func;

  /* clang-format off */
  switch (flags) {
    case 0x01: shared_scan_func = shared_scan_0x01; break;
    case 0x02: shared_scan_func = shared_scan_0x02; break;
    case 0x03: shared_scan_func = shared_scan_0x03; break;
    case 0x04: shared_scan_func = shared_scan_0x04; break;
    case 0x05: shared_scan_func = shared_scan_0x05; break;
    case 0x06: shared_scan_func = shared_scan_0x06; break;
    case 0x07: shared_scan_func = shared_scan_0x07; break;
    case 0x08: shared_scan_func = shared_scan_0x08; break;
    case 0x09: shared_scan_func = shared_scan_0x09; break;
    case 0x0a: shared_scan_func = shared_scan_0x0a; break;
    case 0x0b: shared_scan_func = shared_scan_0x0b; break;
    case 0x0c: shared_scan_func = shared_scan_0x0c; break;
    case 0x0d: shared_scan_func = shared_scan_0x0d; break;
    case 0x0e: shared_scan_func = shared_scan_0x0e; break;
    case 0x0f: shared_scan_func = shared_scan_0x0f; break;
    default: assert(0 && "Invalid flags");
  }
  /* clang-format on */

  // Perform the shared scan either sequentially or in parallel
  if (__multi_threaded__) {
    return _shared_scan_parallel(shared_scan_func, valvec, posvec, ctx, flags);
  }
  return _shared_scan_sequential(shared_scan_func, valvec, posvec, ctx, flags);
}
